import os
import sys
import glob
import time
import json
import hashlib
import uuid
import uvicorn
from typing import List, Dict, Any, Optional, Union
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from llama_cpp import Llama

# APIãƒªã‚¯ã‚¨ã‚¹ãƒˆç”¨ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆOpenAIäº’æ›ã®ã¿ï¼‰
# OpenAI APIäº’æ›ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    max_tokens: Optional[int] = Field(default=10000)
    temperature: Optional[float] = Field(default=0.0)
    top_p: Optional[float] = Field(default=0.9)

# Structured Outputsç”¨ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
class StructuredChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    response_format: Optional[Union[Dict[str, Any], BaseModel]] = None
    max_tokens: Optional[int] = Field(default=10000)
    temperature: Optional[float] = Field(default=0.0)
    top_p: Optional[float] = Field(default=0.9)
    seed: Optional[int] = Field(default=0)

# ãƒ¢ãƒ‡ãƒ«æƒ…å ±ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”¨ã®ãƒ¢ãƒ‡ãƒ«
class ModelInfo(BaseModel):
    name: str
    path: str
    size_mb: float

class ModelsResponse(BaseModel):
    models: List[ModelInfo]

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ä½œæˆ
app = FastAPI(
    title="Backend Server",
    description="ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼ç”¨ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒãƒ¼",
    version="1.0.0"
)

# CORSãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã®è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
models_dir = ""
llama_model: Optional[Llama] = None
current_model_path = ""

def initialize_server(models_directory: str, num_threads: int):
    global models_dir
    models_dir = models_directory
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    if not os.path.isdir(models_dir):
        raise ValueError(f"ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {models_dir}")
    
    print(f"âœ… Backend Server initialized")
    print(f"   ğŸ“ Models directory: {models_dir}")
    print(f"   ğŸ§µ Threads: {num_threads}")

def load_model(model_path: str, num_threads: int) -> Llama:
    global llama_model, current_model_path
    
    # æ—¢ã«åŒã˜ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
    if llama_model is not None and current_model_path == model_path:
        return llama_model
    
    # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    print(f"ğŸ”„ Loading model: {os.path.basename(model_path)}")
    llama_model = Llama(
        model_path=model_path,
        n_ctx=32768,            # inportant: ã“ã“ã§ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨æ™‚ã®max_contextã‚’èª¿æ•´å¯èƒ½
        n_threads=num_threads,
        verbose=False
    )
    current_model_path = model_path
    print(f"âœ… Model loaded successfully")
    return llama_model

# ãƒ«ãƒ¼ãƒˆ
@app.get("/")
async def root():
    return {"message": "LlamaAPI ã‚µãƒ¼ãƒãƒ¼ãŒå®Ÿè¡Œä¸­ã§ã™"}

# OpenAI APIäº’æ›ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/v1")
async def v1_root():
    return {
        "object": "api",
        "version": "v1",
        "message": "LlamaAPI Backend Server - OpenAI Compatible"
    }

@app.get("/v1/models")
async def v1_list_models():
    global models_dir
    
    models = []
    model_files = glob.glob(os.path.join(models_dir, "*.gguf"))
    
    for model_path in model_files:
        model_name = os.path.basename(model_path)
        models.append({
            "id": model_name,
            "object": "model",
            "created": int(time.time()),
            "owned_by": "llamaapi",
            "permission": [],
            "root": model_name,
            "parent": None
        })
    
    return {
        "object": "list",
        "data": models
    }

@app.post("/v1/chat/completions")
async def v1_chat_completions(request: ChatCompletionRequest):
    global models_dir
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®æ§‹ç¯‰
    model_path = os.path.join(models_dir, request.model)
    
    # ãƒ¢ãƒ‡ãƒ«ã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(model_path):
        raise HTTPException(status_code=404, detail=f"ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {request.model}")
    
    try:
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        prompt = ""
        for msg in request.messages:
            if msg.role == "system":
                prompt += f"System: {msg.content}\n"
            elif msg.role == "user":
                prompt += f"User: {msg.content}\n"
        
        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        model = load_model(model_path, num_threads=1)
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        result = model.create_completion(
            prompt=prompt,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p,
            stream=False
        )
        
        # OpenAI APIå½¢å¼ã§è¿”å´
        return {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": request.model,
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": result["choices"][0]["text"]
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": len(prompt.split()),
                "completion_tokens": len(result["choices"][0]["text"].split()),
                "total_tokens": len(prompt.split()) + len(result["choices"][0]["text"].split())
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")

def extract_pydantic_schema(response_format: Union[Dict[str, Any], BaseModel]) -> Dict[str, Any]:
    if isinstance(response_format, dict):
        return response_format
    
    # Pydanticãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€JSONã‚¹ã‚­ãƒ¼ãƒã‚’ç”Ÿæˆ
    if hasattr(response_format, 'model_json_schema'):
        schema = response_format.model_json_schema()
        return {
            "type": "json_schema",
            "json_schema": {
                "name": response_format.__name__.lower(),
                "schema": schema
            }
        }
    
    return response_format

def generate_system_fingerprint(model_name: str, seed: int) -> str:
    content = f"{model_name}_{seed}_{int(time.time() // 3600)}"  # 1æ™‚é–“ã”ã¨ã«å¤‰ã‚ã‚‹
    return f"fp_{hashlib.md5(content.encode()).hexdigest()[:12]}"

def estimate_tokens(text: str) -> int:
    return max(1, len(text.split()) + len(text) // 4)

@app.post("/v1/beta/chat/completions/parse")
async def v1_beta_chat_completions_parse(request: StructuredChatCompletionRequest):
    global models_dir
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®æ§‹ç¯‰
    model_path = os.path.join(models_dir, request.model)
    
    # ãƒ¢ãƒ‡ãƒ«ã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(model_path):
        raise HTTPException(status_code=404, detail=f"ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {request.model}")
    
    try:
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        prompt = ""
        for msg in request.messages:
            if msg.role == "system":
                prompt += f"System: {msg.content}\n"
            elif msg.role == "user":
                prompt += f"User: {msg.content}\n"
        
        # æ§‹é€ åŒ–å‡ºåŠ›ã®æŒ‡ç¤ºã‚’è¿½åŠ 
        if request.response_format:
            schema_info = extract_pydantic_schema(request.response_format)
            prompt += "\nPlease respond in the following JSON format:\n"
            if isinstance(schema_info, dict) and "json_schema" in schema_info:
                prompt += f"Schema: {json.dumps(schema_info['json_schema']['schema'], indent=2)}\n"
            else:
                prompt += f"{json.dumps(schema_info, indent=2)}\n"
            prompt += "Respond with valid JSON only, no additional text.\n"
        
        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        model = load_model(model_path, num_threads=1)
        
        # seedã‚’ä½¿ã£ãŸå†ç¾å¯èƒ½ãªç”Ÿæˆã®ãŸã‚ã€seedã‚’è¨­å®š
        temperature = request.temperature if request.temperature is not None else 0.0
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        result = model.create_completion(
            prompt=prompt,
            max_tokens=request.max_tokens,
            temperature=temperature,
            top_p=request.top_p,
            stream=False
        )
        
        generated_content = result["choices"][0]["text"].strip()
        
        # æ§‹é€ åŒ–å‡ºåŠ›ã®å ´åˆã€JSONè§£æã‚’è©¦è¡Œ
        parsed_content = None
        if request.response_format:
            try:
                # JSONã®é–‹å§‹ã¨çµ‚äº†ã‚’è¦‹ã¤ã‘ã¦æŠ½å‡º
                json_start = generated_content.find('{')
                json_end = generated_content.rfind('}') + 1
                if json_start != -1 and json_end > json_start:
                    json_str = generated_content[json_start:json_end]
                    parsed_content = json.loads(json_str)
                    # ç”Ÿæˆã•ã‚ŒãŸJSONã‚’æ•´å½¢ã—ã¦å†è¨­å®š
                    generated_content = json.dumps(parsed_content, ensure_ascii=False, indent=None)
                else:
                    # JSONãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€å…¨ä½“ã‚’JSONã¨ã—ã¦è§£æã‚’è©¦è¡Œ
                    parsed_content = json.loads(generated_content)
            except json.JSONDecodeError:
                # JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨
                parsed_content = {"content": generated_content}
                generated_content = json.dumps(parsed_content, ensure_ascii=False)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®—
        prompt_tokens = estimate_tokens(prompt)
        completion_tokens = estimate_tokens(generated_content)
        total_tokens = prompt_tokens + completion_tokens
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚£ãƒ³gerprintç”Ÿæˆ
        system_fingerprint = generate_system_fingerprint(request.model, request.seed or 0)
        
        # OpenAI beta.chat.completions.parse å½¢å¼ã§è¿”å´
        completion_id = f"chatcmpl-{uuid.uuid4().hex[:24]}"
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹ç¯‰ï¼ˆmodel_dump_jsonå¯¾å¿œï¼‰
        message_obj = {
            "role": "assistant",
            "content": generated_content,
        }
        
        # æ§‹é€ åŒ–å‡ºåŠ›ã®å ´åˆã€parsed ã¨ refusal ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
        if request.response_format and parsed_content is not None:
            message_obj["parsed"] = parsed_content
            message_obj["refusal"] = None
        
        response = {
            "id": completion_id,
            "object": "chat.completion",
            "created": int(time.time()),
            "model": request.model,
            "system_fingerprint": system_fingerprint,
            "choices": [
                {
                    "index": 0,
                    "message": message_obj,
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": total_tokens,
                "prompt_tokens_details": {
                    "cached_tokens": 0  # Hawkså¯¾å¿œã®ãŸã‚ã«è¿½åŠ 
                }
            }
        }
        
        return response
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")

# ã‚µãƒ¼ãƒãƒ¼èµ·å‹•é–¢æ•°
def start_server(models_directory: str, host: str = "127.0.0.1", port: int = 8080, 
                num_threads: int = 1):
    """ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•"""
    try:
        initialize_server(models_directory, num_threads)
        # CPUä½¿ç”¨ç‡ã‚’æœ€å°åŒ–ã™ã‚‹è¨­å®š
        uvicorn.run(
            app, 
            host=host, 
            port=port,
            workers=1,              # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’1ã«åˆ¶é™
            loop="asyncio",         # asyncioãƒ«ãƒ¼ãƒ—ã‚’ä½¿ç”¨
            access_log=False,       # ã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°ã‚’ç„¡åŠ¹åŒ–
            log_level="error"       # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®ã¿å‡ºåŠ›
        )
    except KeyboardInterrupt:
        print("ğŸ›‘ Server stopped by user")
    except Exception as e:
        print(f"âŒ Server error: {e}")

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print(f"ä½¿ç”¨æ–¹æ³•: python {sys.argv[0]} <ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª> [ãƒ›ã‚¹ãƒˆ=127.0.0.1] [ãƒãƒ¼ãƒˆ=8080] [ã‚¹ãƒ¬ãƒƒãƒ‰æ•°=1]")
        print(f"ä¾‹: python {sys.argv[0]} ./models")
        print(f"ä¾‹: python {sys.argv[0]} ./models 127.0.0.1 8080 1")
        sys.exit(1)
    
    models_directory = sys.argv[1]
    host = sys.argv[2] if len(sys.argv) > 2 else "127.0.0.1"
    port = int(sys.argv[3]) if len(sys.argv) > 3 else 8080
    num_threads = int(sys.argv[4]) if len(sys.argv) > 4 else 1
    
    print("="*50)
    print("ğŸ–¥ï¸  Backend Server")
    print("="*50)
    print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {models_directory}")
    print(f"ğŸŒ ãƒ›ã‚¹ãƒˆ: {host}")
    print(f"ğŸ”Œ ãƒãƒ¼ãƒˆ: {port}")
    print(f"ğŸ§µ ã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {num_threads}")
    print("="*50)
    
    start_server(models_directory, host, port, num_threads) 