import os
import sys
import glob
import time
import json
import hashlib
import uuid
import uvicorn
from typing import List, Dict, Any, Optional, Union
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from llama_cpp import Llama, LlamaGrammar

# APIãƒªã‚¯ã‚¨ã‚¹ãƒˆç”¨ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆOpenAIäº’æ›ã®ã¿ï¼‰
# OpenAI APIäº’æ›ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    max_tokens: Optional[int] = Field(default=10000)
    temperature: Optional[float] = Field(default=0.0)
    top_p: Optional[float] = Field(default=0.9)

# Structured Outputsç”¨ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
class StructuredChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    response_format: Optional[Union[Dict[str, Any], BaseModel]] = None
    max_tokens: Optional[int] = Field(default=10000)
    temperature: Optional[float] = Field(default=0.0)
    top_p: Optional[float] = Field(default=0.9)
    seed: Optional[int] = Field(default=0)

# ãƒ¢ãƒ‡ãƒ«æƒ…å ±ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”¨ã®ãƒ¢ãƒ‡ãƒ«
class ModelInfo(BaseModel):
    name: str
    path: str
    size_mb: float

class ModelsResponse(BaseModel):
    models: List[ModelInfo]

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ä½œæˆ
app = FastAPI(
    title="Backend Server",
    description="ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼ç”¨ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒãƒ¼",
    version="1.0.0"
)

# CORSãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã®è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
models_dir = ""
llama_model: Optional[Llama] = None
current_model_path = ""

def initialize_server(models_directory: str, num_threads: int):
    global models_dir
    models_dir = models_directory
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    if not os.path.isdir(models_dir):
        raise ValueError(f"ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {models_dir}")
    
    print(f"âœ… Backend Server initialized")
    print(f"   ğŸ“ Models directory: {models_dir}")
    print(f"   ğŸ§µ Threads: {num_threads}")

def load_model(model_path: str, num_threads: int) -> Llama:
    global llama_model, current_model_path
    
    # æ—¢ã«åŒã˜ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
    if llama_model is not None and current_model_path == model_path:
        return llama_model
    
    # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    print(f"ğŸ”„ Loading model: {os.path.basename(model_path)}")
    llama_model = Llama(
        model_path=model_path,
        n_ctx=4096,             # æ§‹é€ åŒ–å‡ºåŠ›ã«é©ã—ãŸã‚µã‚¤ã‚ºã«èª¿æ•´
        n_threads=num_threads,
        verbose=False
    )
    current_model_path = model_path
    print(f"âœ… Model loaded successfully")
    return llama_model

# ãƒ«ãƒ¼ãƒˆ
@app.get("/")
async def root():
    return {"message": "LlamaAPI ã‚µãƒ¼ãƒãƒ¼ãŒå®Ÿè¡Œä¸­ã§ã™"}

# OpenAI APIäº’æ›ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/v1")
async def v1_root():
    return {
        "object": "api",
        "version": "v1",
        "message": "LlamaAPI Backend Server - OpenAI Compatible"
    }

@app.get("/v1/models")
async def v1_list_models():
    global models_dir
    
    models = []
    model_files = glob.glob(os.path.join(models_dir, "*.gguf"))
    
    for model_path in model_files:
        model_name = os.path.basename(model_path)
        models.append({
            "id": model_name,
            "object": "model",
            "created": int(time.time()),
            "owned_by": "llamaapi",
            "permission": [],
            "root": model_name,
            "parent": None
        })
    
    return {
        "object": "list",
        "data": models
    }

@app.post("/v1/chat/completions")
async def v1_chat_completions(request: ChatCompletionRequest):
    global models_dir
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®æ§‹ç¯‰
    model_path = os.path.join(models_dir, request.model)
    
    # ãƒ¢ãƒ‡ãƒ«ã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(model_path):
        raise HTTPException(status_code=404, detail=f"ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {request.model}")
    
    try:
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        prompt = ""
        for msg in request.messages:
            if msg.role == "system":
                prompt += f"System: {msg.content}\n"
            elif msg.role == "user":
                prompt += f"User: {msg.content}\n"
        
        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        model = load_model(model_path, num_threads=4)
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        result = model.create_completion(
            prompt=prompt,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p,
            stream=False
        )
        
        # OpenAI APIå½¢å¼ã§è¿”å´
        return {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": request.model,
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": result["choices"][0]["text"]
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": len(prompt.split()),
                "completion_tokens": len(result["choices"][0]["text"].split()),
                "total_tokens": len(prompt.split()) + len(result["choices"][0]["text"].split())
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")

def extract_pydantic_schema(response_format: Union[Dict[str, Any], BaseModel]) -> Dict[str, Any]:
    if isinstance(response_format, dict):
        return response_format
    
    # Pydanticãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€JSONã‚¹ã‚­ãƒ¼ãƒã‚’ç”Ÿæˆ
    if hasattr(response_format, 'model_json_schema'):
        schema = response_format.model_json_schema()
        return {
            "type": "json_schema",
            "json_schema": {
                "name": response_format.__name__.lower(),
                "schema": schema
            }
        }
    
    return response_format

def generate_system_fingerprint(model_name: str, seed: int) -> str:
    content = f"{model_name}_{seed}_{int(time.time() // 3600)}"  # 1æ™‚é–“ã”ã¨ã«å¤‰ã‚ã‚‹
    return f"fp_{hashlib.md5(content.encode()).hexdigest()[:12]}"

def estimate_tokens(text: str) -> int:
    return max(1, len(text.split()) + len(text) // 4)

@app.post("/v1/beta/chat/completions/parse")
async def v1_beta_chat_completions_parse(request: StructuredChatCompletionRequest):
    global models_dir
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®æ§‹ç¯‰
    model_path = os.path.join(models_dir, request.model)
    
    # ãƒ¢ãƒ‡ãƒ«ã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(model_path):
        raise HTTPException(status_code=404, detail=f"ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {request.model}")
    
    try:
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        prompt = ""
        for msg in request.messages:
            if msg.role == "system":
                prompt += f"System: {msg.content}\n"
            elif msg.role == "user":
                prompt += f"User: {msg.content}\n"
        
        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        model = load_model(model_path, num_threads=1)
        
        # seedã‚’ä½¿ã£ãŸå†ç¾å¯èƒ½ãªç”Ÿæˆã®ãŸã‚ã€seedã‚’è¨­å®š
        temperature = request.temperature if request.temperature is not None else 0.0
        
        # æ§‹é€ åŒ–å‡ºåŠ›ã®å ´åˆã€llama-cpp-pythonã®grammaræ©Ÿèƒ½ã‚’ä½¿ç”¨
        if request.response_format and isinstance(request.response_format, dict):
            schema_info = request.response_format
            if "json_schema" in schema_info:
                # JSON Schemaã‹ã‚‰å®Œå…¨ãªGBNFæ–‡æ³•ã‚’ç”Ÿæˆ
                schema = schema_info["json_schema"]["schema"]
                grammar_str = generate_comprehensive_gbnf_grammar(schema)
                
                print(f"ğŸ”§ Schema: {json.dumps(schema, indent=2)}")
                print(f"ğŸ”§ Generated GBNF Grammar:\n{grammar_str}")  # ãƒ‡ãƒãƒƒã‚°ç”¨
                
                # LlamaGrammarã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                try:
                    grammar = LlamaGrammar.from_string(grammar_str)
                    print(f"âœ… LlamaGrammar object created successfully")
                except Exception as grammar_creation_error:
                    print(f"âŒ Failed to create LlamaGrammar: {grammar_creation_error}")
                    raise grammar_creation_error
                
                # Grammaråˆ¶ç´„ä»˜ãã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ä¸è¦ï¼‰
                try:
                    result = model.create_completion(
                        prompt=prompt,
                        max_tokens=min(request.max_tokens or 1000, 1000),
                        temperature=temperature,
                        top_p=request.top_p,
                        grammar=grammar,  # LlamaGrammarã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ç”¨
                        stream=False
                    )
                    generated_content = result["choices"][0]["text"].strip()
                    print(f"âœ… Generated content: {generated_content}")
                    
                    # Grammaråˆ¶ç´„ã«ã‚ˆã‚Šã€ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯å¿…ãšæœ‰åŠ¹ãªJSON
                    try:
                        parsed_content = json.loads(generated_content)
                        print(f"âœ… Successfully parsed JSON: {parsed_content}")
                    except json.JSONDecodeError as e:
                        # Grammaråˆ¶ç´„ãŒã‚ã‚‹ãŸã‚ã€ã“ã‚Œã¯é€šå¸¸ç™ºç”Ÿã—ãªã„ã¯ãš
                        print(f"âš ï¸ Unexpected JSON parse error: {e}")
                        print(f"âš ï¸ Generated content: {generated_content}")
                        parsed_content = {"error": "Grammar constraint failed", "content": generated_content}
                        generated_content = json.dumps(parsed_content, ensure_ascii=False)
                        
                except Exception as grammar_error:
                    print(f"âŒ Grammar generation error: {grammar_error}")
                    # Grammarå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    result = model.create_completion(
                        prompt=prompt + "\nRespond with valid JSON only.",
                        max_tokens=min(request.max_tokens or 1000, 1000),
                        temperature=temperature,
                        top_p=request.top_p,
                        stream=False
                    )
                    generated_content = result["choices"][0]["text"].strip()
                    try:
                        parsed_content = json.loads(generated_content)
                    except json.JSONDecodeError:
                        parsed_content = {"error": "Fallback failed", "content": generated_content}
            else:
                # é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                result = model.create_completion(
                    prompt=prompt,
                    max_tokens=min(request.max_tokens or 1000, 1000),
                    temperature=temperature,
                    top_p=request.top_p,
                    stream=False
                )
                generated_content = result["choices"][0]["text"].strip()
                parsed_content = None
        else:
            # é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            result = model.create_completion(
                prompt=prompt,
                max_tokens=min(request.max_tokens or 1000, 1000),
                temperature=temperature,
                top_p=request.top_p,
                stream=False
            )
            generated_content = result["choices"][0]["text"].strip()
            parsed_content = None
        
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®—
        prompt_tokens = estimate_tokens(prompt)
        completion_tokens = estimate_tokens(generated_content)
        total_tokens = prompt_tokens + completion_tokens
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚£ãƒ³gerprintç”Ÿæˆ
        system_fingerprint = generate_system_fingerprint(request.model, request.seed or 0)
        
        # OpenAI beta.chat.completions.parse å½¢å¼ã§è¿”å´
        completion_id = f"chatcmpl-{uuid.uuid4().hex[:24]}"
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹ç¯‰ï¼ˆmodel_dump_jsonå¯¾å¿œï¼‰
        message_obj = {
            "role": "assistant",
            "content": generated_content,
        }
        
        # æ§‹é€ åŒ–å‡ºåŠ›ã®å ´åˆã€parsed ã¨ refusal ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
        if request.response_format and parsed_content is not None:
            message_obj["parsed"] = parsed_content
            message_obj["refusal"] = None
        
        response = {
            "id": completion_id,
            "object": "chat.completion",
            "created": int(time.time()),
            "model": request.model,
            "system_fingerprint": system_fingerprint,
            "choices": [
                {
                    "index": 0,
                    "message": message_obj,
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": total_tokens,
                "prompt_tokens_details": {
                    "cached_tokens": 0  # Hawkså¯¾å¿œã®ãŸã‚ã«è¿½åŠ 
                }
            }
        }
        
        return response
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")

def generate_comprehensive_gbnf_grammar(schema):
    """JSON Schemaã‹ã‚‰å®Œå…¨ãªGBNFæ–‡æ³•ã‚’ç”Ÿæˆ"""
    
    def generate_property_grammar(prop_schema, prop_name=None):
        """å€‹åˆ¥ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã®GBNFæ–‡æ³•ã‚’ç”Ÿæˆ"""
        prop_type = prop_schema.get("type", "string")
        
        if prop_type == "string":
            if "enum" in prop_schema:
                # Enumæ–‡å­—åˆ—ã®å ´åˆ
                enum_values = prop_schema["enum"]
                enum_rules = " | ".join([f'"\\"" "{value}" "\\""' for value in enum_values])
                return f"({enum_rules})"
            else:
                return "string"
        elif prop_type == "number" or prop_type == "integer":
            return "number"
        elif prop_type == "boolean":
            return "boolean"
        elif prop_type == "array":
            items_schema = prop_schema.get("items", {"type": "string"})
            if "enum" in items_schema:
                # Enumé…åˆ—ã®å ´åˆã¯ç‰¹åˆ¥å‡¦ç†
                return "enum-array"
            else:
                item_rule = generate_property_grammar(items_schema)
                return f"array-{item_rule.replace('-', '_')}"
        elif prop_type == "object":
            # ãƒã‚¹ãƒˆã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆç°¡ç•¥åŒ–ï¼‰
            return "nested-object"
        else:
            return "string"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    if schema.get("type") == "object":
        properties = schema.get("properties", {})
        required = schema.get("required", [])
        
        if not properties:
            # ç©ºã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            return '''
root ::= "{" ws "}"
ws ::= [ \\t\\n]*
'''
        
        # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£å®šç¾©ã‚’ç”Ÿæˆ
        property_rules = []
        for i, (key, prop_schema) in enumerate(properties.items()):
            prop_rule = generate_property_grammar(prop_schema, key)
            property_rules.append(f'"\\"" "{key}" "\\"" ws ":" ws {prop_rule}')
        
        # ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ«æ§‹ç¯‰
        if len(property_rules) == 1:
            # å˜ä¸€ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
            grammar = f'root ::= "{{" ws {property_rules[0]} ws "}}"'
        else:
            # è¤‡æ•°ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
            properties_rule = " ws \",\" ws ".join(property_rules)
            grammar = f'root ::= "{{" ws {properties_rule} ws "}}"'
        
        # åŸºæœ¬ãƒ«ãƒ¼ãƒ«å®šç¾©
        grammar += '''
ws ::= [ \\t\\n]*
string ::= "\\"" [^"\\\\]* "\\""
number ::= "-"? [0-9]+ ("." [0-9]+)?
boolean ::= "true" | "false"
nested-object ::= "{" ws "}"
'''
        
        # é…åˆ—ãƒ«ãƒ¼ãƒ«ã‚’å‹•çš„ã«è¿½åŠ 
        for prop_schema in properties.values():
            if prop_schema.get("type") == "array":
                items_schema = prop_schema.get("items", {"type": "string"})
                item_type = items_schema.get("type", "string")
                
                if "enum" in items_schema:
                    # Enumé…åˆ—ã®å ´åˆ
                    enum_values = items_schema["enum"]
                    enum_rules = " | ".join([f'"\\"" "{value}" "\\""' for value in enum_values])
                    grammar += f'''
enum-array ::= "[" ws (({enum_rules}) (ws "," ws ({enum_rules}))*)? ws "]"
'''
                elif item_type == "string":
                    grammar += '''
array-string ::= "[" ws (string (ws "," ws string)*)? ws "]"
'''
                elif item_type == "number":
                    grammar += '''
array-number ::= "[" ws (number (ws "," ws number)*)? ws "]"
'''
        
        return grammar
    
    elif schema.get("type") == "array":
        items_schema = schema.get("items", {"type": "string"})
        item_type = items_schema.get("type", "string")
        
        if item_type == "string":
            if "enum" in items_schema:
                enum_values = items_schema["enum"]
                enum_rules = " | ".join([f'"\\"" "{value}" "\\""' for value in enum_values])
                return f'''
root ::= "[" ws (({enum_rules}) (ws "," ws ({enum_rules}))*)? ws "]"
ws ::= [ \\t\\n]*
'''
            else:
                return '''
root ::= "[" ws (string (ws "," ws string)*)? ws "]"
ws ::= [ \\t\\n]*
string ::= "\\"" [^"\\\\]* "\\""
'''
        elif item_type == "number":
            return '''
root ::= "[" ws (number (ws "," ws number)*)? ws "]"
ws ::= [ \\t\\n]*
number ::= "-"? [0-9]+ ("." [0-9]+)?
'''
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®JSONæ–‡æ³•
    return '''
root ::= "{" ws "}"
ws ::= [ \\t\\n]*
'''

# ã‚µãƒ¼ãƒãƒ¼èµ·å‹•é–¢æ•°
def start_server(models_directory: str, host: str = "127.0.0.1", port: int = 8080, 
                num_threads: int = 4):
    """ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•"""
    try:
        initialize_server(models_directory, num_threads)
        # CPUä½¿ç”¨ç‡ã‚’æœ€å°åŒ–ã™ã‚‹è¨­å®š
        uvicorn.run(
            app, 
            host=host, 
            port=port,
            workers=1,              # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’1ã«åˆ¶é™
            loop="asyncio",         # asyncioãƒ«ãƒ¼ãƒ—ã‚’ä½¿ç”¨
            access_log=False,       # ã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°ã‚’ç„¡åŠ¹åŒ–
            log_level="error"       # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®ã¿å‡ºåŠ›
        )
    except KeyboardInterrupt:
        print("ğŸ›‘ Server stopped by user")
    except Exception as e:
        print(f"âŒ Server error: {e}")

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print(f"ä½¿ç”¨æ–¹æ³•: python {sys.argv[0]} <ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª> [ãƒ›ã‚¹ãƒˆ=127.0.0.1] [ãƒãƒ¼ãƒˆ=8080] [ã‚¹ãƒ¬ãƒƒãƒ‰æ•°=1]")
        print(f"ä¾‹: python {sys.argv[0]} ./models")
        print(f"ä¾‹: python {sys.argv[0]} ./models 127.0.0.1 8080 1")
        sys.exit(1)
    
    models_directory = sys.argv[1]
    host = sys.argv[2] if len(sys.argv) > 2 else "127.0.0.1"
    port = int(sys.argv[3]) if len(sys.argv) > 3 else 8080
    num_threads = int(sys.argv[4]) if len(sys.argv) > 4 else 4
    
    print("="*50)
    print("ğŸ–¥ï¸  Backend Server")
    print("="*50)
    print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {models_directory}")
    print(f"ğŸŒ ãƒ›ã‚¹ãƒˆ: {host}")
    print(f"ğŸ”Œ ãƒãƒ¼ãƒˆ: {port}")
    print(f"ğŸ§µ ã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {num_threads}")
    print("="*50)
    
    start_server(models_directory, host, port, num_threads) 