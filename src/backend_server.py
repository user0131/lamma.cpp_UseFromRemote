#!/usr/bin/env python3
"""
Backend Server for Load Balancer System
ã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒãƒ¼ï¼ˆãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼ç”¨ï¼‰
"""

import os
import sys
import glob
import uvicorn
from typing import List, Dict, Any, Optional
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from llama_cpp import Llama

# APIãƒªã‚¯ã‚¨ã‚¹ãƒˆç”¨ã®ãƒ¢ãƒ‡ãƒ«
class GenerationRequest(BaseModel):
    prompt: str
    model_name: str
    max_tokens: Optional[int] = Field(default=100, ge=1, le=32768)
    temperature: float = Field(default=0.8, ge=0, le=2.0)
    top_k: int = Field(default=40, ge=1, le=100)
    top_p: float = Field(default=0.9, ge=0, le=1.0)

# APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”¨ã®ãƒ¢ãƒ‡ãƒ«
class GenerationResponse(BaseModel):
    response: str

# ãƒ¢ãƒ‡ãƒ«æƒ…å ±ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”¨ã®ãƒ¢ãƒ‡ãƒ«
class ModelInfo(BaseModel):
    name: str
    path: str
    size_mb: float

class ModelsResponse(BaseModel):
    models: List[ModelInfo]

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ä½œæˆ
app = FastAPI(
    title="Backend Server",
    description="ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼ç”¨ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒãƒ¼",
    version="1.0.0"
)

# CORSãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã®è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
models_dir = ""
llama_model: Optional[Llama] = None
current_model_path = ""

def initialize_server(models_directory: str, num_threads: int):
    """ã‚µãƒ¼ãƒãƒ¼ã®åˆæœŸåŒ–"""
    global models_dir
    models_dir = models_directory
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    if not os.path.isdir(models_dir):
        raise ValueError(f"ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {models_dir}")
    
    print(f"âœ… Backend Server initialized")
    print(f"   ğŸ“ Models directory: {models_dir}")
    print(f"   ğŸ§µ Threads: {num_threads}")

def load_model(model_path: str, num_threads: int) -> Llama:
    """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãï¼‰"""
    global llama_model, current_model_path
    
    # æ—¢ã«åŒã˜ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
    if llama_model is not None and current_model_path == model_path:
        return llama_model
    
    # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    print(f"ğŸ”„ Loading model: {os.path.basename(model_path)}")
    llama_model = Llama(
        model_path=model_path,
        n_ctx=2048,
        n_threads=num_threads,
        verbose=False
    )
    current_model_path = model_path
    print(f"âœ… Model loaded successfully")
    return llama_model

# ãƒ«ãƒ¼ãƒˆ
@app.get("/")
async def root():
    return {"message": "ComeAPI ã‚µãƒ¼ãƒãƒ¼ãŒå®Ÿè¡Œä¸­ã§ã™"}

# ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã®å–å¾—
@app.get("/models", response_model=ModelsResponse)
async def list_models():
    global models_dir
    
    models = []
    model_files = glob.glob(os.path.join(models_dir, "*.gguf"))
    
    for model_path in model_files:
        model_name = os.path.basename(model_path)
        model_size_mb = os.path.getsize(model_path) / (1024 * 1024)
        models.append(ModelInfo(
            name=model_name,
            path=model_path,
            size_mb=round(model_size_mb, 2)
        ))
    
    return ModelsResponse(models=models)

# ãƒ†ã‚­ã‚¹ãƒˆç”ŸæˆAPI
@app.post("/generate", response_model=GenerationResponse)
async def generate_text(request: GenerationRequest):
    global models_dir
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®æ§‹ç¯‰
    model_path = os.path.join(models_dir, request.model_name)
    
    # ãƒ¢ãƒ‡ãƒ«ã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(model_path):
        raise HTTPException(status_code=404, detail=f"ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {request.model_name}")
    
    try:
        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        model = load_model(model_path, num_threads=1)  # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¯1ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®‰å®šå‹•ä½œ
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        result = model.create_completion(
            prompt=request.prompt,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            top_k=request.top_k,
            top_p=request.top_p,
            stream=False
        )
        
        # çµæœã®å–å¾—
        generated_text = result["choices"][0]["text"]
        return GenerationResponse(response=generated_text)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")

# ã‚µãƒ¼ãƒãƒ¼èµ·å‹•é–¢æ•°
def start_server(models_directory: str, host: str = "127.0.0.1", port: int = 8080, 
                num_threads: int = 1):
    """ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•"""
    try:
        initialize_server(models_directory, num_threads)
        # CPUä½¿ç”¨ç‡ã‚’æœ€å°åŒ–ã™ã‚‹è¨­å®š
        uvicorn.run(
            app, 
            host=host, 
            port=port,
            workers=1,              # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’1ã«åˆ¶é™
            loop="asyncio",         # asyncioãƒ«ãƒ¼ãƒ—ã‚’ä½¿ç”¨
            access_log=False,       # ã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°ã‚’ç„¡åŠ¹åŒ–
            log_level="error"       # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®ã¿å‡ºåŠ›
        )
    except KeyboardInterrupt:
        print("ğŸ›‘ Server stopped by user")
    except Exception as e:
        print(f"âŒ Server error: {e}")

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print(f"ä½¿ç”¨æ–¹æ³•: python {sys.argv[0]} <ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª> [ãƒ›ã‚¹ãƒˆ=127.0.0.1] [ãƒãƒ¼ãƒˆ=8080] [ã‚¹ãƒ¬ãƒƒãƒ‰æ•°=1]")
        print(f"ä¾‹: python {sys.argv[0]} ./models")
        print(f"ä¾‹: python {sys.argv[0]} ./models 127.0.0.1 8080 1")
        sys.exit(1)
    
    models_directory = sys.argv[1]
    host = sys.argv[2] if len(sys.argv) > 2 else "127.0.0.1"
    port = int(sys.argv[3]) if len(sys.argv) > 3 else 8080
    num_threads = int(sys.argv[4]) if len(sys.argv) > 4 else 1
    
    print("="*50)
    print("ğŸ–¥ï¸  Backend Server")
    print("="*50)
    print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {models_directory}")
    print(f"ğŸŒ ãƒ›ã‚¹ãƒˆ: {host}")
    print(f"ğŸ”Œ ãƒãƒ¼ãƒˆ: {port}")
    print(f"ğŸ§µ ã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {num_threads}")
    print("="*50)
    
    start_server(models_directory, host, port, num_threads) 